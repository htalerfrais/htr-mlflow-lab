experiment_name: "Qwen2.5-VL OCR (local lines)"
run_name: "qwen2.5-vl-7b-instruct_local_lines"

pipeline: "line_to_text"

dataset:
  name: "local_lines"
  images_dir: "data_local/perso_dataset/hector_pages_lines_3_test/lines_out_sorted"
  ground_truth_path: "data_local/perso_dataset/hector_pages_lines_3_test/gt_hector_test.json"
  image_template: "*line_{id:04d}.*"

preprocessor:
  - "identity"

model:
  name: "qwen_vl"
  pretrained_model_name: "Qwen/Qwen2.5-VL-7B-Instruct"
  # For big models, "auto" typically works best (requires a working HF setup).
  device_map: "auto"
  torch_dtype: "auto"
  max_new_tokens: 256
  prompt: "Read and transcribe all text in the image exactly. Output only the transcription."

